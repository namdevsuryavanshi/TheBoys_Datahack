{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "title = str(\"ANGER ISSUES | ANIMATION STORY |  RG BUCKET LIST\")\n"
      ],
      "metadata": {
        "id": "xDqv_kIp_MRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag = str(['animated story', 'funny animation', 'rg bucket list', 'indian animation', 'angry prash', 'kirtichow', 'funny', 'funny cartoon', 'cartoons', 'memes', 'funny animated memes', 'funny animated movie', 'funny animated', 'funny anim', 'animation movies 2020', 'aikagi the animation'])"
      ],
      "metadata": {
        "id": "GYWhajHxAK3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = title + tag"
      ],
      "metadata": {
        "id": "pkNXV0h-IjVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the Wikipedia page you want to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/Entertainment\"\n",
        "\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find the HTML element that contains the main content of the page\n",
        "content = soup.find(id='mw-content-text').get_text()\n",
        "\n",
        "# Print the content as a string\n",
        "string_1 = (str(content))\n"
      ],
      "metadata": {
        "id": "75lIJTvtANQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the Wikipedia page you want to scrape\n",
        "url1 = \"https://en.wikipedia.org/wiki/Film\"\n",
        "\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url1)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find the HTML element that contains the main content of the page\n",
        "content = soup.find(id='mw-content-text').get_text()\n",
        "\n",
        "# Print the content as a string\n",
        "string_2 = (str(content))\n"
      ],
      "metadata": {
        "id": "HiKgJEjGH3Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDysijzpI_Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string2 = string_1 + string_2"
      ],
      "metadata": {
        "id": "rjYxMqxsH7Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "common_words = len(set(string1.split()) & set(string2.split()))\n",
        "print(common_words)  # Output: 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J92_WLIII6D7",
        "outputId": "28688bb6-1d47-44df-f824-0e78837a43bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    }
  ]
}